stringy_relevants <- paste(stringy_relevants, ' \n ', '"', row.names(relevant)[i], '"', ' appears ', relevant[i, ], ' times', sep = '')
}
cli_alert(str_glue('In {fileName}, the following relevant terms were found: {stringy_relevants}'))
print(kable(relevant, caption = str_glue('Relevant terms for {fileName}')))
# N-grams----------------------------------------------------------------------
cli_alert_info(str_glue('{rtime()} -- N-grams'))
# Corpus to df
text_df <- data.frame(line = 1:length(corp_stop), text = sapply(corp_stop, as.character))
# Tokenize in bigrams
bigrams <- text_df |> unnest_tokens(bigram, text, token = 'ngrams', n = 2)
suppressWarnings({
# Bigram df
bigrams_df <- bigrams |>
mutate(
clean_bigrams = unaccent(bigram)
) |>
get_rid('bigram') |>
separate(clean_bigrams, c('word1', 'word2', sep = ' ')) |>
get_rid(' ') |>
filter(word1 != word2)
})
cli_alert(str_glue('{rtime()} -- Counting and filtering'))
# Count bigrams
total_bigrams <- nrow(bigrams_df)
unique_word_count <- count(bigrams_df, word1)
bigram_count <- count(bigrams_df, word1, word2, sort = T)
# Get bigrams with key words
key_bigrams <- bigram_count |>
filter(word1 %in% key | word2 %in% key) |>
arrange(desc(n))
cli_alert(str_glue('{rtime()} -- {fileName} contains {nrow(key_bigrams)} relevant bigrams, with "{key_bigrams$word1[1]} {key_bigrams$word2[1]}" (n = {key_bigrams$n[1]}) being the most frequent'))
print(kable(head(key_bigrams, n = 15), caption = str_glue('Relevant bigrams for {fileName}')))
# PMI--------------------------------------------------------------------------
cli_alert_info(str_glue('{rtime()} -- Instrumentalization of density'))
# Calculate PMI
pmi_df <- left_join(bigram_count, unique_word_count, by = c('word1' = 'word1'))
pmi_df <- left_join(pmi_df, unique_word_count, by = c('word2' = 'word1')) |>
rename(
n_bigram = n.x,
n_w1 = n.y,
n_w2 = n
) |>
na.omit() |>
mutate(
p_bigram = n_bigram / total_bigrams,
p_w1 = n_w1 / total_bigrams,
p_w2 = n_w2 / total_bigrams,
pmi = log2(p_bigram / (p_w1 * p_w2))
)
# look for keywords
relevant_pmi <- pmi_df |>
filter(word1 %in% key | word2 %in% key) |>
arrange(-pmi)
View(m)
View(tdm)
View(m)
inspect(tdm)
?removePunctuation
# As TermDocumentMatrix
tdm <- TermDocumentMatrix(
corp_stop,
control = list(removePunctuation = TRUE, stopwords = TRUE, removeNumbers = TRUE, tolower = TRUE)
)
inspect(tdm)
# As matrix
m <- as.matrix(tdm)
class(corp_stop)
clean_tdm <- funcion(corpus) {
clean_tdm <- function(corpus) {
clean_tdm <- function(corpus) {
for (i in seq_along(corpus)) {
doc <- corpus[[i]]$content
clean_doc <- unaccent(gsub('[^a-zA-Z]', '', doc))
corpus[[i]]$content <- clean_doc
}
return(corpus)
}
# Read file
sp_file <- read.delim('frecuencia_formas_ortograficas_1_0.txt')
# Notes------------------------------------------------------------------------
# Please uncomment lines 18 & 19 for the first execution unless you know for a fact that the packages `rtoolbox` (dev package), `remotes` and `xfun` are installed. I recommend re-commenting them after the first execution in order to avoid errors or warnings.
# Please remember to replace your working directory in line 29 and your file name in line 37.
# Libs-------------------------------------------------------------------------
# install.packages(c('xfun', 'remotes'))
# remotes::install_github('mrosas47/rtoolbox')
suppressWarnings({
xfun::pkg_attach(c('dplyr', 'stringr', 'pdftools', 'cli', 'tm', 'tidytext', 'tidyr', 'scraEP', 'rtoolbox', 'knitr'), message = F, install = T)
})
# Functions--------------------------------------------------------------------
context <- function(body, word, n) {
context_list <- str_extract_all(
as.character(body),
paste(
'.{0,',
as.character(n),
'}',
as.character(word),
'.{0,',
as.character(n),
'}',
sep = ''
)
)
return(context_list)
}
# Defs-------------------------------------------------------------------------
# Ensure working directory
wdEnsure('N:/work/OCUC/nlp')
# Keywords
# key <- unaccent(c('verticalización', 'densificación', 'densidad', 'renovación', 'crecimiento', 'vertical', 'verticalidad', 'verticalizar', 'crecer', 'densificar', 'renovar', 'altura', 'constructibilidad', 'edificabilidad', 'volumen', 'crecimiento'))
key <- 'densidad'
# Spanish language (list of common words)
cli_alert(str_glue('{rtime()} -- Making Spanish language word list'))
# Read file
sp_file <- read.delim('frecuencia_formas_ortograficas_1_0.txt')
View(sp_file)
sp_file <- read.delim('frecuencia_formas_ortograficas_1_0.txt')
# Make list
sp <- c()
for (e in str_replace_all(str_replace_all(tolower(unaccent(sp_file$Forma)), '[^[:alnum:]]', ''), problems, '')) {
if (e %notin% sp) {
sp <- unique(c(sp, e))
}
}
# Problematic symbols
problems <- c('‘', '’', '“', '”', '„', '°')
sp_file <- read.delim('frecuencia_formas_ortograficas_1_0.txt')
# Make list
sp <- c()
for (e in str_replace_all(str_replace_all(tolower(unaccent(sp_file$Forma)), '[^[:alnum:]]', ''), problems, '')) {
if (e %notin% sp) {
sp <- unique(c(sp, e))
}
}
victor <- unaccent(sp_file$Forma)
victor <- tolower(unaccent(sp_file$Forma))
victor[sample(1:nrow(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor <- str_replace_all(tolower(unaccent(sp_file$Forma)), '[^[:alnum:]]', '')
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
is.numeric('661')
as.numeric('661')
as.numeric('ribedii')
victor <- str_replace_all(tolower(unaccent(sp_file$Forma)), '[^[:alnum:]]', '')
for (x in problems) {
victor <- str_replace_all(victor, x, '')
}
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
sp_file <- read.delim('frecuencia_formas_ortograficas_1_0.txt')
victor <- str_replace_all(tolower(unaccent(sp_file$Forma)), '[^[:alnum:]]', '')
for (x in problems) {
if (is.na(as.numeric(x))) {
victor <- str_replace_all(victor, x, '')
}
}
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
victor[sample(1:length(victor), 1)]
sp_file <- read.delim('frecuencia_formas_ortograficas_1_0.txt')
victor <- str_replace_all(tolower(unaccent(sp_file$Forma)), '[^[:alnum:]]', '')
for (x in problems) {
if (is.na(as.numeric(x))) {
victor <- str_replace_all(victor, x, '')
}
}
# Make list
sp <- c()
for (e in victor) {
if (e %notin% sp) {
sp <- unique(c(sp, e))
}
}
v <- as.vector(unique(victor))
length(v)
sp_file <- read.delim('frecuencia_formas_ortograficas_1_0.txt')
victor <- str_replace_all(tolower(unaccent(sp_file$Forma)), '[^[:alnum:]]', '')
for (x in problems) {
if (is.na(as.numeric(x))) {
victor <- str_replace_all(victor, x, '')
}
}
v <- as.vector(unique(victor))
# Make list
sp <- c()
for (e in victor) {
if (e %notin% sp) {
sp <- unique(c(sp, e))
}
}
str_sub(victor, 1, 10)
str_sub(v, 1, 10)
v <- as.vector(unique(victor))[is.na(as.numeric(as.vector(unique(victor))))]
str_sub(v, 1, 10)
write.table(v, 'test.txt')
write.table(v, 'test.txt', row.names = F, col.names = F)
t <- read.delim('test.txt')
write.table(v, 'test.txt')
t <- read.delim('test.txt')
View(t)
gc()
# Meta-------------------------------------------------------------------------
# Title: NLP testing
# Project: FONDECYT Anatomías de verticalización
# URL: NA
# Author: Martín Rosas Araya
# Contact: admin@mra-portal.dev | mirosas@uc.cl
# Last mod date: 07/11/23
# Notes------------------------------------------------------------------------
# Please uncomment lines 18 & 19 for the first execution unless you know for a fact that the packages `rtoolbox` (dev package), `remotes` and `xfun` are installed. I recommend re-commenting them after the first execution in order to avoid errors or warnings.
# Please remember to replace your working directory in line 29 and your file name in line 37.
# Libs-------------------------------------------------------------------------
# install.packages(c('xfun', 'remotes'))
# remotes::install_github('mrosas47/rtoolbox')
suppressWarnings({
xfun::pkg_attach(c('dplyr', 'stringr', 'pdftools', 'cli', 'tm', 'tidytext', 'tidyr', 'scraEP', 'rtoolbox', 'knitr'), message = F, install = T)
})
# Functions--------------------------------------------------------------------
context <- function(body, word, n) {
context_list <- str_extract_all(
as.character(body),
paste(
'.{0,',
as.character(n),
'}',
as.character(word),
'.{0,',
as.character(n),
'}',
sep = ''
)
)
return(context_list)
}
# Defs-------------------------------------------------------------------------
# Ensure working directory
wdEnsure('N:/work/OCUC/nlp')
# Keywords
# key <- unaccent(c('verticalización', 'densificación', 'densidad', 'renovación', 'crecimiento', 'vertical', 'verticalidad', 'verticalizar', 'crecer', 'densificar', 'renovar', 'altura', 'constructibilidad', 'edificabilidad', 'volumen', 'crecimiento'))
key <- 'densidad'
# Problematic symbols
problems <- c('‘', '’', '“', '”', '„', '°')
# Spanish language (list of common words)
cli_alert(str_glue('{rtime()} -- Making Spanish language word list'))
# Read file
sp_file <- read.delim('frecuencia_formas_ortograficas_1_0.txt')
victor <- str_replace_all(tolower(unaccent(sp_file$Forma)), '[^[:alnum:]]', '')
for (x in problems) {
victor <- str_replace_all(victor, x, '')
}
v <- as.vector(unique(victor))[is.na(as.numeric(as.vector(unique(victor))))]
suppressWarnings({
xfun::pkg_attach(c('dplyr', 'stringr', 'pdftools', 'cli', 'tm', 'tidytext', 'tidyr', 'scraEP', 'rtoolbox', 'knitr', 'qdapDictionaries'), message = F, install = T)
})
en <- GradyAugmented
en
tempdir()
gc()
source("N:/work/OCUC/nlp/nlp.R")
# Get context for word `w`
ctx <- context(full_text, w, 50)
ctx
all_words <- str_split(ctx, ' ')
all_words
valid_words <- all_words[all_words != w]
valid_words
all_words[all_words != w]
all_words[[1]]
valid_words <- all_words[[1]][all_words != w]
valid_words
valid_words[14] == w
v <- c()
for (i in valid_words) {if (i != w) {v <- c(v, i)}}
v <- c()
for (i in all_words) {if (i != w) {v <- c(v, i)}}
all_words <- str_split(ctx, ' ')[[1]]
valid_words <- all_words[[1]][all_words != w]
gc()
source("N:/work/OCUC/nlp/nlp.R")
# Get context for word `w`
ctx <- context(full_text, w, 50)
all_words <- str_split(ctx, ' ')[[1]]
valid_words <- all_words[[1]][all_words != w]
valid_words
all_words
valid_words <- all_words[all_words != w]
valid_words
source("N:/work/OCUC/nlp/nlp.R")
View(relevant)
cli_h2(str_glue('{rtime()} -- Operationalization of context'))
# Full text content as a single string
full_text <- unaccent(paste(unlist(sapply(corp_stop, as.character)), collapse = ' '))
for (w in unaccent(names(relevant))) {
cli_alert_info(str_glue('{rtime()} -- Getting context for {w}'))
# Get context for word `w`
ctx <- context(full_text, w, 50)
all_words <- str_split(ctx, ' ')[[1]]
valid_words <- all_words[all_words != w]
# Check if real words
real_words <- valid_words[valid_words %in% sp | valid_words %in% en]
# Write keyword-specific tempfiles
write.table(real_words, str_glue('temp/{fileName}/{w}.txt'), row.names = F, col.names = F)
}
names(relevant)
row.names(relevant)
cli_h2(str_glue('{rtime()} -- Operationalization of context'))
# Full text content as a single string
full_text <- unaccent(paste(unlist(sapply(corp_stop, as.character)), collapse = ' '))
for (w in unaccent(row.names(relevant))) {
cli_alert_info(str_glue('{rtime()} -- Getting context for {w}'))
# Get context for word `w`
ctx <- context(full_text, w, 50)
all_words <- str_split(ctx, ' ')[[1]]
valid_words <- all_words[all_words != w]
# Check if real words
real_words <- valid_words[valid_words %in% sp | valid_words %in% en]
# Write keyword-specific tempfiles
write.table(real_words, str_glue('temp/{fileName}/{w}.txt'), row.names = F, col.names = F)
}
cli_h2(str_glue('{rtime()} -- Operationalization of context'))
# Full text content as a single string
full_text <- unaccent(paste(unlist(sapply(corp_stop, as.character)), collapse = ' '))
for (w in unaccent(row.names(relevant))) {
cli_alert_info(str_glue('{rtime()} -- Getting context for {w}'))
# Get context for word `w`
ctx <- context(full_text, w, 50)
all_words <- str_split(ctx, ' ')[[1]]
valid_words <- all_words[all_words != w & all_words != '']
# Check if real words
real_words <- valid_words[valid_words %in% sp | valid_words %in% en]
# Write keyword-specific tempfiles
suppressWarnings({
write.table(real_words, str_glue('temp/{fileName}/{w}.txt'), row.names = F, col.names = F)
})
}
d <- read.delim('temp/test_this.pdf/densifiacion.txt')
clean_name <- filepath_sans_ext(fileName)
clean_name <- filepath(fileName, 'clean')
?file.path
full_text <- unaccent(paste(unlist(sapply(corp_stop, as.character)), collapse = ' '))
for (w in unaccent(row.names(relevant))) {
cli_alert_info(str_glue('{rtime()} -- Getting context for {w}'))
# Get context for word `w`
ctx <- context(full_text, w, 50)
all_words <- str_split(ctx, ' ')[[1]]
valid_words <- all_words[all_words != w & all_words != '']
# Check if real words
real_words <- valid_words[valid_words %in% sp | valid_words %in% en]
# Write keyword-specific tempfiles
suppressWarnings({
write.table(real_words, str_glue('temp/{fileName}/{w}.txt'), row.names = F, col.names = F)
})
}
clean_name <- filepath(fileName, 'clean')
clean_name <- file.path(fileName, 'clean')
clean_name <- basename(fileName)
clean_name <- file_sans_ext(fileName)
xfun::pkg_attach(c('dplyr', 'stringr', 'pdftools', 'cli', 'tm', 'tidytext', 'tidyr', 'scraEP', 'rtoolbox', 'knitr', 'qdapDictionaries', 'tools'), message = F, install = T)
clean_name <- file_sans_ext(fileName)
clean_name <- file_path_sans_ext(fileName)
gc()
source("N:/work/OCUC/nlp/nlp.R")
full_text <- unaccent(paste(unlist(sapply(corp_stop, as.character)), collapse = ' '))
for (w in unaccent(row.names(relevant))) {
cli_alert_info(str_glue('{rtime()} -- Getting context for {w}'))
# Get context for word `w`
ctx <- context(full_text, w, 50)
suppressWarnings({
all_words <- str_split(ctx, ' ')[[1]]
})
valid_words <- all_words[all_words != w & all_words != '']
# Check if real words
real_words <- valid_words[valid_words %in% sp | valid_words %in% en]
# Write keyword-specific tempfiles
write.table(real_words, str_glue('temp/{clean_name}/{w}.txt'), row.names = F, col.names = F)
}
t <- read.delim('temp/test_this/densificacion.txt')
View(t)
t <- read.delim('temp/test_this/densificacion.txt', col.names = F)
View(t)
?read.demil
?read.delim
t <- read.delim('temp/test_this/densificacion.txt', header = F)
View(t)
t <- read.delim('temp/test_this/densificacion.txt', header = F, col.names = 'word')
View(t)
# Unique word count
unique_word_sum <- length(unique(real_words))
View(unique_word_count)
ctx
valid_words
all_words <- str_split(unlist(ctx, ' '))
all_words <- str_split(unlist(ctx), ' ')
View(all_words)
rm(all_words)
all_words <- str_split(unlist(ctx), ' ')
all_words[[1]]
all_words[[2]]
suppressWarnings({
all_words <- unlist(ctx)
})
all_words
suppressWarnings({
all_words <- str_split(unlist(ctx), ' ')
})
all_words <- as.vector(str_split(unlist(ctx), ' '))
all_words <- str_split(ctx, ' ')
View(all_words)
all_words <- str_split(ctx, ' ')[[1]]
all_words
?str_replace_all
valid_words <- str_replace_all(all_words[all_words %notin% c(w, '', 'c(\"')], c('c(', '\"', ')'), '', ','), '')
valid_words <- str_replace_all(all_words[all_words %notin% c(w, '', 'c(\"')], c('c(', '\"', ')', ','), '')
valid_words <- str_replace_all(all_words[all_words %notin% c(w, '', 'c(\"')], any(c('c(', '\"', ')', ',')), '')
ctx <- context(full_text, w, 50)
suppressWarnings({
all_words <- strsplit(ctx, ' ')
})
all_words <- str_split(ctx, ' ')
all_words <- unlist(str_split(ctx, ' '))
valid_words <- all_words[all_words %notin% c(w, '', 'c(\"')]
valid_words
syms <- c('\"', 'c(\"', ',')
y <- lapply(syms, str_replace_all, string = valid_words, pattern = ., replacement = '')
y <- lapply(syms, function(sym) str_replace_all(string = valid_words, pattern = sym, replacement = ''))
syms <- c('\"', ',')
y <- lapply(syms, function(sym) str_replace_all(string = valid_words, pattern = sym, replacement = ''))
y
syms <- c('\"', ',', ')', '(')
y <- lapply(syms, function(sym) str_replace_all(string = valid_words, pattern = sym, replacement = ''))
syms <- c('\"', ',', ')', '`(`')
y <- lapply(syms, function(sym) str_replace_all(string = valid_words, pattern = sym, replacement = ''))
syms <- c('\"', ',', ')', `'('``)
valid_words <- str_replace_all(tolower(unaccent(all_words)), '[^[:alnum:]]', '')
valid_words
valid_words <- valid_words[valid_words %notin% c(w, '', 'c')]
valid_words
# Check if real words
real_words <- valid_words[valid_words %in% sp | valid_words %in% en]
real_words
gc()
source("N:/work/OCUC/nlp/nlp.R")
# Unique word count
unique_word_sum <- sum(unique(real_words))
# Unique word count
unique_word_sum <- table(unique(real_words))
View(unique_word_count)
real_words
# Unique word count
unique_word_sum <- table(real_words)
View(unique_word_count)
# Unique word count
unique_word_sum <- unique(real_words)
table(unique_word_count)
real_words
table(real_words)
# Unique word count
unique_word_sum <- table(real_words)
unique_word_sum
# Unique word count
unique_word_sum <- as.data.frame(table(real_words))
View(unique_word_sum)
source("N:/work/OCUC/nlp/nlp.R")
